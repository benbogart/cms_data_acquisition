{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Scrape-URLs\" data-toc-modified-id=\"Scrape-URLs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Scrape URLs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base-Page\" data-toc-modified-id=\"Base-Page-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Base Page</a></span></li><li><span><a href=\"#Collect-all-URLs-for-each-report\" data-toc-modified-id=\"Collect-all-URLs-for-each-report-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Collect all URLs for each report</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sanity-Check\" data-toc-modified-id=\"Sanity-Check-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Sanity Check</a></span></li></ul></li></ul></li><li><span><a href=\"#Download-and-extract-files\" data-toc-modified-id=\"Download-and-extract-files-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Download and extract files</a></span></li><li><span><a href=\"#File-Types\" data-toc-modified-id=\"File-Types-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>File Types</a></span><ul class=\"toc-item\"><li><span><a href=\"#zip-files\" data-toc-modified-id=\"zip-files-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>zip files</a></span></li><li><span><a href=\"#txt-files\" data-toc-modified-id=\"txt-files-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>txt files</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start requests cache:\n",
    "requests_cache.install_cache('cms_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape URLs\n",
    "## Base Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://www.cms.gov/'\n",
    "start_url = 'https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(start_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/Monthly-Contract-and-Enrollment-Summary-Report',\n",
       " '/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/Monthly-Enrollment-by-Contract',\n",
       " '/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/Monthly-Enrollment-by-Contract-Plan-State-County',\n",
       " '/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/Monthly-Enrollment-by-Plan',\n",
       " '/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/Monthly-Enrollment-by-State']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_menu = soup.find(\"div\", {\"id\": 'block-cmsmainnavigation'})\n",
    "reports = left_menu.findAll(\"li\", {\"class\": 'menu-item'})\n",
    "report_urls = [r.find(\"a\")['href'] for r in reports]\n",
    "report_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that there are 26 report urls\n",
    "len(report_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all URLs for each report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_soup(url, params = {'items_per_page':100}): \n",
    "    r = requests.get(url,params=params)    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def extract_num_entries(soup):\n",
    "    #regex, or BS?\n",
    "    entries_text = soup.find('span', {'class': 'ds-l-sm-col--12'}).text.strip()\n",
    "    num_entries = entries_text.split(' ')[-2]    \n",
    "    return int(num_entries)\n",
    "\n",
    "def extract_monthly_report_urls(soup):\n",
    "    \n",
    "    # extract table rows\n",
    "    table = soup.find('div', {'class':'view-content'})\n",
    "    body = soup.find('tbody')\n",
    "    rows = body.findAll('tr')\n",
    "    \n",
    "    # local data storage object\n",
    "    reports = []\n",
    "    \n",
    "    # extract row wise data\n",
    "    for row in rows:\n",
    "        cells = row.findAll('td')\n",
    "        \n",
    "        report = {'report_period':cells[1].text,\n",
    "                  'report_title':cells[0].text,\n",
    "                  'report_url':cells[0].a['href']}\n",
    "        reports.append(report)\n",
    "    \n",
    "    return reports\n",
    "\n",
    "def extract_downloads(soup):\n",
    "    # extract table rows\n",
    "    rows = soup.findAll('div', {'class':'media--view-mode-file-list'})\n",
    "    \n",
    "    # local data storage object\n",
    "    reports = {'report_period':[],\n",
    "               'report_title':[],\n",
    "               'download_url':[]}\n",
    "    \n",
    "    # extract row wise data\n",
    "    for row in rows:\n",
    "\n",
    "        reports['report_period'] = np.nan\n",
    "        reports['report_title'].append(row.text.strip())\n",
    "        reports['download_url'].append(row.a['href'])\n",
    "    \n",
    "    # return as DataFrame\n",
    "    df = pd.DataFrame(reports)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_page_title(soup):\n",
    "    return soup.find(id='skipNavTarget').text.strip()\n",
    "\n",
    "def extract_description(soup):\n",
    "    return soup.find('div', {'id':'block-cms-drupal-global-content'}).div.p.text\n",
    "\n",
    "def is_download_page(soup):\n",
    "    if soup.find('h2', {'class':'field__label'}):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_links_list_pate(soup):\n",
    "    \n",
    "    if soup.find('span', {'class': 'ds-l-sm-col--12'}):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_all_pages(url, recursion=0, verbose=True):\n",
    "    # page can be select with `page` and `items_per_page` params\n",
    "    \n",
    "    if verbose:\n",
    "        print('Accessing', url)\n",
    "        print('--> recursion:', recursion)\n",
    "\n",
    "    params = {'items_per_page':100}\n",
    "    soup = get_page_soup(url, params)\n",
    "    \n",
    "    # test for download page\n",
    "    #if is_download_page(soup):\n",
    "    if not is_links_list_pate(soup):\n",
    "        if verbose:\n",
    "            print('--> extracting downloads')\n",
    "        \n",
    "        df = extract_downloads(soup)\n",
    "        df['download_page_url'] = url\n",
    "        return df\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('--> getting links to download pages')\n",
    "        # get number of entries\n",
    "        num_entries = extract_num_entries(soup)\n",
    "\n",
    "        # get first page\n",
    "        reports = extract_monthly_report_urls(soup)    \n",
    "\n",
    "        # recurse to get all pages of download urls\n",
    "        # 0 indexed page id\n",
    "        page = 0\n",
    "        while len(reports) < num_entries:\n",
    "            page += 1\n",
    "            params['page'] = page\n",
    "            soup = get_page_soup(url, params)\n",
    "            reports += extract_monthly_report_urls(soup)\n",
    "                \n",
    "        dfs = []\n",
    "        for report in reports:\n",
    "            next_url = urljoin(start_url, report['report_url'])\n",
    "            dfs.append(get_all_pages(next_url, recursion+1, verbose))\n",
    "            \n",
    "        df = pd.concat(dfs)\n",
    "        \n",
    "        df['description'] = extract_description(soup)\n",
    "        df['page_title'] = extract_page_title(soup)\n",
    "        df['page_url'] = url\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df = get_all_pages('https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/MCRAdvPartDEnrolData/SNP-HEDIS-Public-Use-Files', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing https://www.cms.gov/research-statistics-data-and-systemsstatistics-trends-and-reportsmcradvpartdenroldatamonthly/contract-summary-2020-12\n",
      "--> recursion: 0\n",
      "--> extracting downloads\n"
     ]
    }
   ],
   "source": [
    "test_df = get_all_pages('https://www.cms.gov/research-statistics-data-and-systemsstatistics-trends-and-reportsmcradvpartdenroldatamonthly/contract-summary-2020-12', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_period</th>\n",
       "      <th>report_title</th>\n",
       "      <th>download_url</th>\n",
       "      <th>download_page_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Monthly Contract Summary Report – December 202...</td>\n",
       "      <td>/files/zip/monthly-contract-summary-report-dec...</td>\n",
       "      <td>https://www.cms.gov/research-statistics-data-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   report_period                                       report_title  \\\n",
       "0            NaN  Monthly Contract Summary Report – December 202...   \n",
       "\n",
       "                                        download_url  \\\n",
       "0  /files/zip/monthly-contract-summary-report-dec...   \n",
       "\n",
       "                                   download_page_url  \n",
       "0  https://www.cms.gov/research-statistics-data-a...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get separate dataframes of all the report urls.\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e38afa32967a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreport_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreport_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_url' is not defined"
     ]
    }
   ],
   "source": [
    "for report_url in tqdm(report_urls):\n",
    "    url=urljoin(base_url,report_url)\n",
    "    dfs.append(get_all_pages(url, verbose=False))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_urls = []\n",
    "for df in dfs:\n",
    "    visited_urls += df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_url'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['download_url'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to look into why there are so many report titles and what that means for storing these reports in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/cms-file-links.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extract files\n",
    "\n",
    "Download and extract files in a single step to avoid overtaxing the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test download first file.\n",
    "df = pd.read_csv('data/cms-file-links.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_filename_from_url(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def download_file(url, save_path='files', verbose=False):\n",
    "    \n",
    "    # create files dir if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    filename = pop_filename_from_url(url)\n",
    "    file_path = os.path.join(save_path, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        if verbose:\n",
    "            print(filename, 'already exists')\n",
    "    else:\n",
    "        r = requests.get(url)\n",
    "        \n",
    "        # error if we didn't get the file\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        file = open(file_path, \"wb\")\n",
    "        file.write(r.content)\n",
    "        file.close()\n",
    "        if verbose:\n",
    "            print(filename, 'downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_FILES_PATH = 'files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(df.download_url):\n",
    "    url = urljoin(start_url, url)\n",
    "    download_file(url, DOWNLOAD_FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(os.listdir(DOWNLOAD_FILES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two filename errors that repeat:\n",
    "- files ending in zip-0 (this could indicate multi-part zip files)\n",
    "- extension missing `.` separator\n",
    "\n",
    "Below we deal with these by creating a custom splitext function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_zip(filename):\n",
    "    base, ext = os.path.splitext(filename) \n",
    "    \n",
    "    if ext == '.zip' or ext == '.zip-0':\n",
    "        return True\n",
    "    elif filename.endswith('zip'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def splitext(filename):\n",
    "\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    \n",
    "    # missing .\n",
    "    if ext == '' and filename.endswith('zip'):\n",
    "        ext = '.' + filename[-3:]\n",
    "        return filename[:-4], ext \n",
    "    \n",
    "    if ext == '.zip-0':\n",
    "        ext = '.zip'\n",
    "    return base, ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTED_ZIP_FILES_PATH = 'zip_extract'\n",
    "\n",
    "# unzip\n",
    "notzip = []\n",
    "\n",
    "for url in tqdm(df.download_url):\n",
    "    filename = pop_filename_from_url(url)\n",
    "    \n",
    "    report_name, ext = splitext(filename) \n",
    "        \n",
    "    # print(report_name, ext)\n",
    "    if ext == '.zip':\n",
    "        extract_path = os.path.join(EXTRACTED_ZIP_FILES_PATH, report_name)\n",
    "\n",
    "        # print(f'unzipping {filename} to {extract_path}')\n",
    "        \n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "            \n",
    "            file_path = os.path.join(DOWNLOAD_FILES_PATH, filename)\n",
    "            \n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "#         else:\n",
    "#             print(filename, 'already extracted')\n",
    "    else:\n",
    "        # treat as extracted file and copy to extracted folders dir\n",
    "        os.copy(file_path, extract_path)\n",
    "        notzip.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_counts = {}\n",
    "\n",
    "for (root,dirs,files) in os.walk(EXTRACTED_ZIP_FILES_PATH):\n",
    "    for file in files:\n",
    "        base, ext = os.path.splitext(file)\n",
    "        ext = ext.lower()\n",
    "        file_type_counts[ext] = file_type_counts.get(ext, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_counts = pd.DataFrame([file_type_counts.keys(), file_type_counts.values()]).T\n",
    "ft_counts.columns = ['ext', 'count']\n",
    "ft_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(x='count',\n",
    "            y='ext',\n",
    "            data=ft_counts.sort_values('count'),\n",
    "            color='#E8AE68')\n",
    "\n",
    "plt.title('Number of files by type (from CMS)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip files\n",
    "were the zip files in other zip files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = glob(os.path.join(EXTRACTED_ZIP_FILES_PATH, '**/*.zip'), recursive=True)\n",
    "zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zips:\n",
    "    path, filename = os.path.split(z)\n",
    "    print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files seem to have alternative versions with alt in the filename.  Some exploration is required to find out how the alternative files differ.\n",
    "\n",
    "See how many `alt` files there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts = glob(os.path.join(EXTRACTED_ZIP_FILES_PATH, '**Alt**/*.*'), recursive=True)\n",
    "alts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of alternative files\n",
    "len(alts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## txt files\n",
    "Are the text files delimited tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = glob(os.path.join(EXTRACTED_ZIP_FILES_PATH, '**/*.txt'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = np.random.choice(txts)\n",
    "print(txt)\n",
    "pd.read_csv(np.random.choice(txts), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most txt files seem to be content descriptions\n",
    "pd.read_csv('zip_extract/Monthly-Enrollment-by-Contract-April-2008./readme_monthly_report_by_Contract.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some txt files contain data\n",
    "pd.read_csv('zip_extract/PBP-Benefits-2016./pbp_Section_D_opts.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('zip_extract/PBP-Benefits-2016.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe the sas files are not sas files?\n",
    "pd.read_sas('zip_extract/PBP-Benefits-2016./pbp_Section_D_opt.sas', format='sas7bdat' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:webscraping]",
   "language": "python",
   "name": "conda-env-webscraping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
